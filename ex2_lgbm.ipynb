{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# データの特徴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install optuna optuna-integration\n",
    "\n",
    "# import lightgbm as lgb\n",
    "import optuna.integration.lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import optuna\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file_path = '/Users/shigeyuki-t/Desktop/output.csv'\n",
    "dataset = pd.read_csv(file_path)\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# フォルダ・ファイルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# フォルダ・ファイルの作成\n",
    "remove_name = ['sample1115@gmail.com','test1124@gmail.com','0gt38r252z23g5v@ezweb.ne.jp',\n",
    "               'shigeyuki.taira@gmail.com','kitajima.koki.kp8@naist.ac.jp','0gt38r252z23g5v@ezweb.ne.jp',\n",
    "               'yokota.eiko.yg4@bs.naist.jp','sekiguchi.hiroka.si1@naist.ac.jp','mushaffarasyidridha@gmail.com',\n",
    "              'naka.taisuke.nt3@bs.naist.jp','valeriemegan10@yahoo.com'] #valeriemegan10@yahoo.comは許可取れてない\n",
    "dataset = dataset[~dataset['userEmail'].isin(remove_name)]\n",
    "print(dataset['userEmail'].nunique())\n",
    "print(dataset['userEmail'].value_counts())\n",
    "\n",
    "unique_ids = dataset['userEmail'].unique()\n",
    "save_location = '/Users/shigeyuki-t/Desktop/Experiment_2/analysis/subject/'\n",
    "\n",
    "for userEmail in unique_ids:\n",
    "    # フォルダ作成\n",
    "    folder_name = f'{userEmail}'\n",
    "    folder_path = os.path.join(save_location, folder_name)\n",
    "    \n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f'created:b {folder_path}')\n",
    "    else:\n",
    "        print(f'already exists: {folder_path}')\n",
    "    # csv作成\n",
    "    csv_name =  f'{userEmail}' + '.csv'\n",
    "    csv_path = '/Users/shigeyuki-t/Desktop/Experiment_2/analysis/subject/' + f'{userEmail}' + '/' \n",
    "    mail_dataset = dataset[dataset['userEmail'] == f'{userEmail}']\n",
    "    csv_create_location = os.path.join(csv_path, csv_name)\n",
    "    mail_dataset.to_csv(csv_create_location, index = True)\n",
    "    \n",
    "    if not os.path.exists(csv_create_location):\n",
    "        os.makedirs(csv_create_location)\n",
    "        print(f'created:b {csv_create_location}')\n",
    "    else:\n",
    "        print(f'already exists: {csv_create_location}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ成形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "for userEmail in unique_ids:\n",
    "    user_name = userEmail\n",
    "    file_path = '/Users/shigeyuki-t/Desktop/Experiment_2/analysis/subject/' + f'{user_name}/' + f'{user_name}' + '.csv'\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        refined_data = pd.read_csv(file_path)\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    refined_data = refined_data.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "    # UNIXタイムスタンプをdatetime形式に変換・ソート\n",
    "    refined_data['dateTime'] = pd.to_datetime(refined_data['dateTime'], unit='s')\n",
    "    refined_data = refined_data.sort_values(by='dateTime')\n",
    "    refined_data['dateTime_numeric'] = refined_data['dateTime'].astype('int64') // 10**9  # 1/21追加分\n",
    "\n",
    "    # isCorrectを1/0で分けた後次のtaskの値を参照\n",
    "    refined_data['isCorrect'] = refined_data['isCorrect'].replace({True: 1, False: 0})\n",
    "    tasks = sorted(refined_data['task'].unique())  # taskの順番を取得\n",
    "    for i in range(len(tasks) - 1):\n",
    "        current_task = tasks[i]\n",
    "        next_task = tasks[i + 1]\n",
    "        # 現在のtaskのisCorrectを次のtaskのisCorrectに反映\n",
    "        refined_data.loc[refined_data['task'] == current_task, 'isCorrect'] = \\\n",
    "        refined_data.loc[refined_data['task'] == next_task, 'isCorrect'].values[0]\n",
    "\n",
    "    # 指定された [\"question\"] の [\"isCorrect\"] を反転\n",
    "    flip_questions = [49, 84, 111, 136, 150]\n",
    "    refined_data.loc[refined_data['question'].isin(flip_questions), 'isCorrect'] = \\\n",
    "        refined_data.loc[refined_data['question'].isin(flip_questions), 'isCorrect'].apply(lambda x: 1 - x)\n",
    "\n",
    "    # 指定された [\"question\"] の [\"isCorrect\"] を一律1に変更\n",
    "    set_to_one_questions = [26, 28, 36, 80]\n",
    "    refined_data.loc[refined_data['question'].isin(set_to_one_questions), 'isCorrect'] = 1\n",
    "\n",
    "\n",
    "    min_task = 3\n",
    "    max_task = refined_data['task'].max()\n",
    "    refined_data = refined_data[(refined_data['task'] > min_task) & (refined_data['task'] <= max_task)]\n",
    "\n",
    "    # \"pre_correct_rate\"列の作成\n",
    "    def calculate_pre_correct_rate(refined_data):\n",
    "        # 新しい列を初期化\n",
    "        refined_data['pre_correct_rate'] = 0.0\n",
    "\n",
    "        # 全データを task ごとにループ\n",
    "        tasks = sorted(refined_data['task'].unique())\n",
    "        for n in tasks:\n",
    "            # 現在の task の範囲を計算 (n-4~n)\n",
    "            lower_bound = max(n - 10, min(tasks))  # 下限がデータの最小 task を超えないように調整\n",
    "            upper_bound = n-1\n",
    "\n",
    "            # 該当する範囲のデータを抽出\n",
    "            task_range_data = refined_data[(refined_data['task'] >= lower_bound) & (refined_data['task'] <= upper_bound)]\n",
    "\n",
    "            # \"isCorrect\"==1 の task 群の数を計算\n",
    "            correct_task_count = task_range_data[task_range_data['isCorrect'] == 1]['task'].nunique()\n",
    "\n",
    "            # 範囲内のタスク数を計算\n",
    "            total_task_count = len(task_range_data['task'].unique())\n",
    "\n",
    "            # 正解率を計算 (正解タスク数 / 範囲内のタスク数)\n",
    "            correct_rate = correct_task_count / total_task_count if total_task_count > 0 else 0\n",
    "\n",
    "            # 現在の task の行に対して \"correct_rate\" を設定\n",
    "            refined_data.loc[refined_data['task'] == n, 'pre_correct_rate'] = correct_rate\n",
    "\n",
    "        return refined_data\n",
    "\n",
    "    # correct_rate を計算してデータに追加\n",
    "    refined_data = calculate_pre_correct_rate(refined_data)\n",
    "\n",
    "    # \"future_correct_rate\"列の作成\n",
    "    def calculate_future_correct_rate(refined_data):\n",
    "        # 新しい列を初期化\n",
    "        refined_data['future_correct_rate'] = 0.0\n",
    "\n",
    "        # 全データを task ごとにループ\n",
    "        tasks = sorted(refined_data['task'].unique())\n",
    "        for n in tasks:\n",
    "            # 現在の task の範囲を計算 (n-4~n)\n",
    "            lower_bound = max(n, min(tasks))  # 下限がデータの最小 task を超えないように調整\n",
    "            upper_bound = n+9\n",
    "\n",
    "            # 該当する範囲のデータを抽出\n",
    "            task_range_data = refined_data[(refined_data['task'] >= lower_bound) & (refined_data['task'] <= upper_bound)]\n",
    "\n",
    "            # \"isCorrect\"==1 の task 群の数を計算\n",
    "            correct_task_count = task_range_data[task_range_data['isCorrect'] == 1]['task'].nunique()\n",
    "\n",
    "            # 範囲内のタスク数を計算\n",
    "            total_task_count = len(task_range_data['task'].unique())\n",
    "\n",
    "            # 正解率を計算 (正解タスク数 / 範囲内のタスク数)\n",
    "            correct_rate = correct_task_count / total_task_count if total_task_count > 0 else 0\n",
    "\n",
    "            # 現在の task の行に対して \"correct_rate\" を設定\n",
    "            refined_data.loc[refined_data['task'] == n, 'future_correct_rate'] = correct_rate\n",
    "\n",
    "        return refined_data\n",
    "\n",
    "    # correct_rate を計算してデータに追加\n",
    "    refined_data = calculate_future_correct_rate(refined_data)\n",
    "\n",
    "    print(f\"pre:{refined_data[\"pre_correct_rate\"].head()}\")\n",
    "    print(f\"future:{refined_data[\"future_correct_rate\"].head()}\")\n",
    "\n",
    "    # future_correct_rateに基づいてsatisficingを設定\n",
    "    refined_data[\"satisficing\"] = (refined_data[\"future_correct_rate\"] < 0.4).astype(int)\n",
    "\n",
    "    # カテゴリ変数をダミー変数に変換\n",
    "    refined_data = pd.get_dummies(refined_data, columns=['event'])\n",
    "    required_columns = ['event_response', 'event_scroll', 'event_tapCount']\n",
    "    for column in required_columns:\n",
    "        if column not in refined_data.columns:\n",
    "            refined_data[column] = 0  # 生成されなかったカラムは0で埋める\n",
    "\n",
    "    # NaNを0で埋める\n",
    "    refined_data.fillna(0, inplace=True)\n",
    "    comp_refined_data = refined_data.copy()\n",
    "\n",
    "    # 数値型列を抽出・# 標準化の適用\n",
    "    numeric_columns = refined_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "    numeric_columns = numeric_columns.drop(['task', 'isCorrect','pre_correct_rate','future_correct_rate',\"satisficing\"])  # 'task'と'isCorrect'列を標準化対象から除外\n",
    "    scaler = StandardScaler()\n",
    "    refined_data[numeric_columns] = scaler.fit_transform(refined_data[numeric_columns])\n",
    "\n",
    "    # 標準化後のデータを保存\n",
    "    csv_name = 'feature_' + f'{user_name}' + '.csv'\n",
    "    csv_path = '/Users/shigeyuki-t/Desktop/Experiment_2/analysis/subject/' + f'{user_name}' + '/'\n",
    "    csv_create_location = os.path.join(csv_path, csv_name)\n",
    "    if not os.path.exists(csv_path):\n",
    "        os.makedirs(csv_path)\n",
    "    refined_data.to_csv(csv_create_location, index=True)\n",
    "\n",
    "    print(f'File saved: {csv_create_location}')\n",
    "\n",
    "print(refined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import make_interp_spline\n",
    "#適切回答/不良回答の割合を算出\n",
    "total_bad_count = 0\n",
    "total_good_count = 0\n",
    "\n",
    "for userEmail in unique_ids:\n",
    "    print(userEmail)\n",
    "    file_path = '/Users/shigeyuki-t/Desktop/Experiment_2/analysis/subject/' + f'{userEmail}' + '/feature_' + f'{userEmail}' + '.csv'\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        refined_data = pd.read_csv(file_path)\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    # 平均正答率\n",
    "    unique_tasks = refined_data.drop_duplicates(subset=\"task\")\n",
    "    average_correct_rate = unique_tasks[\"future_correct_rate\"].mean()\n",
    "\n",
    "    # 平均以下のマーク\n",
    "    refined_data = refined_data.merge(unique_tasks[['task', 'future_correct_rate']], on='task', suffixes=('', '_unique'))\n",
    "    refined_data[\"below_average\"] = refined_data[\"future_correct_rate_unique\"] < 0.4\n",
    "\n",
    "    # # スプライン補間による滑らかな折れ線グラフ\n",
    "    # x = unique_tasks[\"task\"]\n",
    "    # y = unique_tasks[\"pre_correct_rate\"]\n",
    "\n",
    "    # # 補間のための新しいx軸データ生成\n",
    "    # x_new = np.linspace(x.min(), x.max(), 300)\n",
    "    # spl = make_interp_spline(x, y, k=3)  # 3次スプライン補間\n",
    "    # y_smooth = spl(x_new)\n",
    "\n",
    "    # Plot task-wise correct rate with below-average points highlighted\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.lineplot(data=unique_tasks, x=\"task\", y=\"pre_correct_rate\", color=\"blue\", label=\"Correct Rate\")#, marker=\"o\"\n",
    "\n",
    "    # Highlight below-average points in red\n",
    "    below_average_data = unique_tasks[unique_tasks[\"future_correct_rate\"] < 0.4]#average_correct_rate\n",
    "    plt.scatter(below_average_data[\"task\"], below_average_data[\"pre_correct_rate\"], color=\"red\", label=\"Below Average\", zorder=5)\n",
    "\n",
    "    # Add a horizontal line for the average correct rate\n",
    "    plt.axhline(y=average_correct_rate, color=\"green\", linestyle=\"--\", label=f\"Average Correct Rate ({average_correct_rate:.2f})\")\n",
    "\n",
    "    # Customize plot aesthetics\n",
    "    plt.title(f\"Task-wise Correct Rate with Average Highlight ({userEmail})\", fontsize=16)\n",
    "    plt.xlabel(\"Task\", fontsize=14)\n",
    "    plt.ylabel(\"Correct Rate\", fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # bad_count = 0\n",
    "    # good_count = 0\n",
    "    # # 全データを task ごとに処理\n",
    "    # tasks = refined_data['task'].unique()\n",
    "    # for task in tasks:\n",
    "    #     # 現在の task に対応するデータを取得\n",
    "    #     task_data = refined_data[refined_data['task'] == task]\n",
    "\n",
    "    #     # isCorrect の 0 (bad) と 1 (good) をそれぞれ判定\n",
    "    #     if (task_data['isCorrect'] == 0).any():\n",
    "    #         bad_count += 1  # bad_count を +1\n",
    "    #     if (task_data['isCorrect'] == 1).any():\n",
    "    #         good_count += 1  # good_count を +1\n",
    "\n",
    "    # total_bad_count += bad_count\n",
    "    # total_good_count += good_count\n",
    "\n",
    "    # # taskごとの正答率推移を表にする処理\n",
    "    # def calculate_correct_rate_trend(refined_data):\n",
    "    #     # taskごとのcorrect_rateの平均を計算\n",
    "    #     correct_rate_trend = refined_data.groupby('task')['correct_rate'].mean().reset_index()\n",
    "\n",
    "    #     # カラム名をわかりやすく変更\n",
    "    #     correct_rate_trend.columns = ['task', 'average_correct_rate']\n",
    "\n",
    "    #     return correct_rate_trend\n",
    "    # taskごとの正答率推移を表にする処理（5の倍数のtaskのみ参照）\n",
    "    # def calculate_correct_rate_trend_for_multiples_of_5(refined_data):\n",
    "    #     # taskごとのcorrect_rateの平均を計算\n",
    "    #     correct_rate_trend = refined_data.groupby('task')['correct_rate'].mean().reset_index()\n",
    "\n",
    "    #     # taskが5の倍数のみフィルタリング\n",
    "    #     correct_rate_trend = correct_rate_trend[correct_rate_trend['task'] % 10 == 0]\n",
    "\n",
    "    #     # カラム名をわかりやすく変更\n",
    "    #     correct_rate_trend.columns = ['task', 'average_correct_rate']\n",
    "\n",
    "    #     return correct_rate_trend\n",
    "\n",
    "    # # 正答率の推移を計算\n",
    "    # correct_rate_trend = calculate_correct_rate_trend_for_multiples_of_5(refined_data)\n",
    "\n",
    "    # # グラフを描画する処理\n",
    "    # def plot_correct_rate_trend(correct_rate_trend, user_name):\n",
    "    #     plt.figure(figsize=(10, 6))\n",
    "    #     sns.lineplot(data=correct_rate_trend, x='task', y='average_correct_rate', marker='o', color='blue', label='Correct Rate')\n",
    "    #     plt.title(f'Task-wise Correct Rate Trend ({user_name})', fontsize=16)\n",
    "    #     plt.xlabel('Task', fontsize=14)\n",
    "    #     plt.ylabel('Average Correct Rate', fontsize=14)\n",
    "    #     plt.xticks(fontsize=12)\n",
    "    #     plt.yticks(fontsize=12)\n",
    "    #     plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    #     plt.legend(fontsize=12)\n",
    "    #     plt.tight_layout()\n",
    "\n",
    "    #     # グラフを保存\n",
    "    #     plot_path = '/Users/shigeyuki-t/Desktop/Experiment_2/analysis/subject/' + f'{user_name}/correct_rate_trend.png'\n",
    "    #     plt.savefig(plot_path)\n",
    "    #     print(f\"Correct rate trend graph saved: {plot_path}\")\n",
    "    #     plt.show()\n",
    "\n",
    "    # # グラフの描画\n",
    "    # plot_correct_rate_trend(correct_rate_trend, user_name)\n",
    "\n",
    "\n",
    "    # 最終的なカウントを出力\n",
    "    # print(f\"Total bad_count: {bad_count}\")\n",
    "    # print(f\"Total good_count: {good_count}\")\n",
    "\n",
    "# print(total_bad_count)\n",
    "# print(total_good_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徴量クラスタリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# # 特徴量によるクラスタリング\n",
    "user_group_data = []\n",
    "\n",
    "user_group = [\n",
    "\"amemiya.naoki.ao9@naist.ac.jp\",\n",
    "\"kasatani.hikaru.kd6@bs.naist.jp\",\n",
    "\"syouta9592@i.softbank.jp\",\n",
    "\"nishida.amane.mw6@ms.naist.jp\",\n",
    "\"bessho.taisei.bu6@gmail.com\",\n",
    "\"maya.sugi0102@gmail.com\",\n",
    "\"sannzasannza@gmail.con\",\n",
    "\"norilemon811@gmail.com\",\n",
    "\"personal@muhammadalqaaf.com\",\n",
    "\"niapotter@gmail.com\",\n",
    "\"rubaet.ema@gmail.com\",\n",
    "\"ito.mana.ik9@ms.naist.jp\",\n",
    "\"oyebodeoyewale065@gmail.com\", \n",
    "\"koyama.honami.kg7@naist.ac.jp\",\n",
    "\"ilya.maisarah_binti_thariq.in9@naist.ac.jp\", \n",
    "\"reru428@moimoi.re\",\n",
    "\"sasaki.rina.su6@g.ext.naist.jp\", \n",
    "\"marina.alki@tum.de\",\n",
    "\"yamamoto.yukino.yv8@ms.naist.jp\", \n",
    "\"li.siyuan.lu1@is.naist.jp\",\n",
    "\"nakamura.kaito.nj3@is.naist.jp\", \n",
    "\"zjnnyyj@outlook.jp\",\n",
    "\"mspamungkas@icloud.com\", \n",
    "\"m.kojima.kn6@naist.ac.jp\",\n",
    "\"micnaist@gmail.com\",\n",
    "\"nandakumar.ardra.na1@bs.naist.jp\",\n",
    "\"tester11_1@gmail.com\"\n",
    "]\n",
    "\n",
    "for sensor_id in unique_ids:\n",
    "    file_path = f'/Users/shigeyuki-t/Desktop/Experiment_2/analysis/subject/{sensor_id}/feature_{sensor_id}.csv'\n",
    "    df = pd.read_csv(file_path, index_col=0)\n",
    "    # 必要な列のみ抽出\n",
    "    remove_feature = ['dateTime', 'userEmail', 'screenW', 'screenH']\n",
    "    df = df.drop(columns=remove_feature, errors='ignore')\n",
    "    df_mean = df.mean()  # 各被験者の特徴量の平均値を計算\n",
    "    user_group_data.append(df_mean)\n",
    "\n",
    "# 特徴量データを1つのデータフレームに統合\n",
    "feature_df = pd.DataFrame(user_group_data, index= unique_ids)\n",
    "\n",
    "# print(type(feature_df))\n",
    "# print(feature_df.isnull().sum())\n",
    "# print(np.isinf(feature_df).sum().sum())\n",
    "\n",
    "# 欠損値や無限大を処理\n",
    "feature_df = feature_df.fillna(0)  # NaNを0で埋める\n",
    "feature_df = feature_df.replace([np.inf, -np.inf], 0)  # 無限大を0で埋める\n",
    "\n",
    "# 標準化\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(feature_df)\n",
    "\n",
    "# k-meansクラスタリングでグループ分け（k=3）\n",
    "n_clusters = 4\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(scaled_features)\n",
    "\n",
    "# グループ情報を出力\n",
    "grouped_users = {i: [] for i in range(n_clusters)}\n",
    "for user, label in zip(user_group, cluster_labels):\n",
    "    grouped_users[label].append(user)\n",
    "\n",
    "print(\"Clustered groups:\")\n",
    "print(grouped_users)\n",
    "\n",
    "# PCAで2次元に縮約\n",
    "pca = PCA(n_components=2)\n",
    "pca_components = pca.fit_transform(scaled_features)\n",
    "\n",
    "# クラスタリング結果の可視化\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(n_clusters):\n",
    "    plt.scatter(pca_components[cluster_labels == i, 0], pca_components[cluster_labels == i, 1], label=f'Cluster {i}', s=50)\n",
    "\n",
    "# 軸ラベルとタイトルの設定\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('K-means Clustering of User Features')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "# 標準化\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(feature_df)\n",
    "\n",
    "# k-distanceプロットを作成\n",
    "neighbors = NearestNeighbors(n_neighbors=5)\n",
    "neighbors_fit = neighbors.fit(scaled_features)\n",
    "distances, indices = neighbors_fit.kneighbors(scaled_features)\n",
    "\n",
    "distances = np.sort(distances[:, 4], axis=0)  # 5番目の最近傍距離を取得\n",
    "plt.plot(distances)\n",
    "plt.xlabel(\"Points sorted by distance\")\n",
    "plt.ylabel(\"5th Nearest Neighbor Distance\")\n",
    "plt.title(\"k-distance Plot\")\n",
    "plt.show()\n",
    "\n",
    "# DBSCANクラスタリング\n",
    "dbscan = DBSCAN(eps=5.0, min_samples=5)  # epsとmin_samplesはデータに応じて調整\n",
    "cluster_labels = dbscan.fit_predict(scaled_features)\n",
    "\n",
    "# クラスタ情報を出力\n",
    "unique_labels = set(cluster_labels)\n",
    "grouped_users_dbscan = {label: [] for label in unique_labels}\n",
    "for user, label in zip(user_group, cluster_labels):\n",
    "    grouped_users_dbscan[label].append(user)\n",
    "\n",
    "print(\"DBSCAN Clusters:\")\n",
    "print(grouped_users_dbscan)\n",
    "\n",
    "# PCAで2次元に縮約\n",
    "pca = PCA(n_components=2)\n",
    "pca_components = pca.fit_transform(scaled_features)\n",
    "\n",
    "# クラスタリング結果の可視化\n",
    "plt.figure(figsize=(8, 6))\n",
    "for label in unique_labels:\n",
    "    if label == -1:\n",
    "        # -1はノイズ（クラスタに属さないデータ）\n",
    "        plt.scatter(pca_components[cluster_labels == label, 0], pca_components[cluster_labels == label, 1],\n",
    "                    label='Noise', color='gray', s=50)\n",
    "    else:\n",
    "        plt.scatter(pca_components[cluster_labels == label, 0], pca_components[cluster_labels == label, 1],\n",
    "                    label=f'Cluster {label}', s=50)\n",
    "\n",
    "# 軸ラベルとタイトルの設定\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('DBSCAN Clustering of User Features')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回答品質によって特徴量の分布が変わるか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回答品質の正常時と低下時の特徴量の分布をか確認\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "group_0 = comp_refined_data[comp_refined_data[\"satisficing\"] == 0]\n",
    "group_1 = comp_refined_data[comp_refined_data[\"satisficing\"]  == 1]\n",
    "\n",
    "# 比較する列名のリスト\n",
    "columns_to_compare = [\"question\", \"rightCount\", \"leftCount\", \"task\", \"pre_correct_rate\"]\n",
    "\n",
    "for column in columns_to_compare:\n",
    "    # group_1 と group_0 の指定した列のデータを抽出\n",
    "    data_group_0 = group_0[column]\n",
    "    data_group_1 = group_1[column]\n",
    "    \n",
    "    \n",
    "    # 箱ひげ図を作成\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    # 中央値を太く\n",
    "    medianprops = dict(color='red', linewidth=2)\n",
    "    \n",
    "    # group_1 と group_0 のデータをまとめて箱ひげ図を作成\n",
    "    ax.boxplot([data_group_0,data_group_1], patch_artist=False,  # 色なし\n",
    "               labels=['Group 0','Group 1'],\n",
    "               medianprops=medianprops)\n",
    "    \n",
    "    # タイトルとラベルを設定\n",
    "    ax.set_title(f'Comparison of \"{column}\" between Group 0 and Group 01')\n",
    "    ax.set_ylabel(column)\n",
    "    ax.grid(False)\n",
    "    \n",
    "    # レイアウトを整えて表示\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル設計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "def objective(trial, train_dataset, test_dataset):\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_error',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 8, 32),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 50, 150),\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 5, 15),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 5, 15),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        # 'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 0.1),\n",
    "        'verbosity': -1,\n",
    "        'early_stopping_rounds':10,\n",
    "        'verbose_eval':False,\n",
    "        \"feature_pre_filter\": False,\n",
    "        \"is_unbalance\": True\n",
    "    }\n",
    "    # モデルの作成と訓練\n",
    "    model = lgb.train(params, train_dataset, num_boost_round=1000, valid_sets=test_dataset)\n",
    "\n",
    "    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    y_pred = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "user_group1 = [\n",
    "\"amemiya.naoki.ao9@naist.ac.jp\",\n",
    "\"kasatani.hikaru.kd6@bs.naist.jp\",\n",
    "\"syouta9592@i.softbank.jp\",\n",
    "\n",
    "\"nishida.amane.mw6@ms.naist.jp\",\n",
    "\"bessho.taisei.bu6@gmail.com\",\n",
    "\"maya.sugi0102@gmail.com\",\n",
    "\"sannzasannza@gmail.con\",\n",
    "\"norilemon811@gmail.com\",\n",
    "\"personal@muhammadalqaaf.com\",\n",
    "\"niapotter@gmail.com\",\n",
    "\"rubaet.ema@gmail.com\",\n",
    "\"ito.mana.ik9@ms.naist.jp\",\n",
    "\"oyebodeoyewale065@gmail.com\",\n",
    "\"koyama.honami.kg7@naist.ac.jp\",\n",
    "\"ilya.maisarah_binti_thariq.in9@naist.ac.jp\",\n",
    "\"reru428@moimoi.re\",\n",
    "\n",
    "\"sasaki.rina.su6@g.ext.naist.jp\",\n",
    "\n",
    "\"marina.alki@tum.de\",\n",
    "\"yamamoto.yukino.yv8@ms.naist.jp\",\n",
    "\n",
    "\"li.siyuan.lu1@is.naist.jp\",\n",
    "\"nakamura.kaito.nj3@is.naist.jp\",\n",
    "\n",
    "\"zjnnyyj@outlook.jp\",\n",
    "\"mspamungkas@icloud.com\",\n",
    "\"m.kojima.kn6@naist.ac.jp\",\n",
    "\"micnaist@gmail.com\",\n",
    "\"nandakumar.ardra.na1@bs.naist.jp\",\n",
    "\"tester11_1@gmail.com\"\n",
    "]\n",
    "\n",
    "cluster_user =[\n",
    "    # 'nishida.amane.mw6@ms.naist.jp', \n",
    "    # 'norilemon811@gmail.com', \n",
    "    # 'personal@muhammadalqaaf.com', \n",
    "    # 'niapotter@gmail.com', \n",
    "    # 'rubaet.ema@gmail.com', \n",
    "    # 'oyebodeoyewale065@gmail.com', \n",
    "    # 'koyama.honami.kg7@naist.ac.jp', \n",
    "    # 'sasaki.rina.su6@g.ext.naist.jp', \n",
    "    # 'marina.alki@tum.de', \n",
    "    # 'nandakumar.ardra.na1@bs.naist.jp'\n",
    "              ]\n",
    "\n",
    "all_user_test = []\n",
    "all_user_pred = []\n",
    "all_user_feature_importances = []\n",
    "\n",
    "for sensor_id in user_group1 :\n",
    "    user_name = sensor_id\n",
    "    file_path = f'/Users/shigeyuki-t/Desktop/Experiment_2/analysis/subject/{user_name}/feature_{user_name}.csv'\n",
    "    df = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "#     print(df.columns)\n",
    "\n",
    "    remove_feature = ['dateTime','userEmail',]# 'question','leftCount','rightCount','screenW', 'screenH'\n",
    "    df = df.drop(columns=remove_feature)\n",
    "\n",
    "    # 欠損値や無限大を処理\n",
    "    df = df.fillna(0)\n",
    "    df = df.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    features = set(df.columns) - {\"satisficing\",'future_correct_rate'}\n",
    "    features = list(features)\n",
    "    target = \"satisficing\"\n",
    "\n",
    "    predictions = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    # 全体の予測結果を保存するリスト\n",
    "    all_y_test = []  # 実際のラベル\n",
    "    all_y_pred = []  # 予測結果\n",
    "    all_feature_importances = []\n",
    "\n",
    "    for n in range(5, 150):\n",
    "        # 'task'列を基準にデータを分割\n",
    "        # train_data = df[df['task'].isin(range(n-10, n+1))]\n",
    "        # if train_data.empty:\n",
    "        #     print(f\"訓練データが空のため、範囲を調整（n={n}）\")\n",
    "        #     train_data = df[df['task'].isin(range(n-(n-1), n+1))]\n",
    "        # test_data = df[df['task'].isin(range(n+1, n+6))]\n",
    "        train_data = df[df['task'].isin(range(n+1))]\n",
    "        test_data = df[df['task']==n+1]\n",
    "\n",
    "        X_train = train_data[features]\n",
    "        y_train = train_data[target]\n",
    "\n",
    "        X_test = test_data[features]\n",
    "        y_test = test_data[target]\n",
    "\n",
    "        # 特徴量やラベルが空の場合はスキップ\n",
    "        if X_train.empty or X_test.empty or y_train.empty or y_test.empty:\n",
    "            continue\n",
    "\n",
    "        # クラスが２つ以上あるか\n",
    "        if len(y_train.unique()) > 1:\n",
    "            # LightGBM用データセットの作成\n",
    "            train_dataset = lgb.Dataset(X_train, label=y_train)\n",
    "            test_dataset = lgb.Dataset(X_test, label=y_test, reference=train_dataset)\n",
    "\n",
    "            study = optuna.create_study(direction='maximize')\n",
    "            study.optimize(lambda trial: objective(trial, train_dataset, test_dataset), n_trials=50)\n",
    "\n",
    "            best_params = study.best_params\n",
    "            best_params.update({'objective': 'binary', 'metric': 'binary_error', 'verbosity': -1})\n",
    "\n",
    "\n",
    "            params = {\n",
    "                'objective': 'binary',\n",
    "                'metric': 'binary_error',\n",
    "                'num_leaves': 16,\n",
    "                'min_data_in_leaf': 100,\n",
    "                'lambda_l1': 10.0,\n",
    "                'lambda_l2': 10.0,\n",
    "                'feature_fraction': 0.8,\n",
    "                'bagging_fraction': 0.8,\n",
    "                'bagging_freq': 5,\n",
    "                'early_stopping_rounds': 10,\n",
    "                'verbosity': -1,\n",
    "                'verbose_eval':False\n",
    "            }\n",
    "\n",
    "            # モデルの作成と訓練\n",
    "            model = lgb.train(best_params, train_dataset, num_boost_round=1000, valid_sets=test_dataset)\n",
    "            #model = lgb.train(params, train_dataset, num_boost_round=1000, valid_sets=test_dataset)\n",
    "\n",
    "            #特徴量重要度を取得\n",
    "            importance = model.feature_importance()\n",
    "            importance_df = pd.DataFrame({'Feature': features, 'Importance': importance})\n",
    "            all_feature_importances.append(importance_df)\n",
    "            all_user_feature_importances.append(importance_df)\n",
    "\n",
    "            # 保存\n",
    "            model_path = '/Users/shigeyuki-t/Desktop/Experiment_2/analysis/subject/' + f'{user_name}/'\n",
    "            if not os.path.exists(model_path):\n",
    "                    os.makedirs(model_path)\n",
    "            model.save_model(os.path.join(model_path, f'model_{n}.txt'))\n",
    "\n",
    "            # 訓練データでの予測（過学習チェック）\n",
    "            y_train_pred = model.predict(X_train, num_iteration=model.best_iteration)\n",
    "            y_train_pred_binary = [1 if pred > 0.5 else 0 for pred in y_train_pred]\n",
    "\n",
    "            # テストデータでの予測\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "            y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "\n",
    "            # 1タスクごとの予測値を追加\n",
    "            all_y_test.append(y_test.tolist()[0])\n",
    "            all_y_pred.append(y_pred_binary[0])\n",
    "            all_user_test.append(y_test.tolist()[0])\n",
    "            all_user_pred.append(y_pred_binary[0])\n",
    "\n",
    "            # 訓練データとテストデータの精度を計算（過学習チェック）\n",
    "            train_accuracy = accuracy_score(y_train, y_train_pred_binary)\n",
    "            test_accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            test_accuracies.append(test_accuracy)\n",
    "\n",
    "        else:\n",
    "            # 最頻値を使う場合\n",
    "            if not y_train.empty and not y_train.mode().empty:\n",
    "                y_pred = [y_train.mode()[0]]\n",
    "            else:\n",
    "                y_pred = [0]\n",
    "                # 予測結果を追加\n",
    "                all_y_test.append(y_test.tolist()[0])\n",
    "                all_y_pred.append(y_pred[0])\n",
    "                all_user_test.append(y_test.tolist()[0])\n",
    "                all_user_pred.append(y_pred[0])\n",
    "\n",
    "        # 予測結果を保存\n",
    "#         predictions.extend(y_pred if len(y_pred) > 0 else [1])\n",
    "        predictions.extend(list(y_pred) if len(y_pred) > 0 else [1])\n",
    "\n",
    "    # 訓練データとテストデータの精度を表示\n",
    "    # print(f\"train_accuracies:{train_accuracies}\")\n",
    "    # ones_count = train_accuracies.count(1.0)\n",
    "    # print(f\"訓練データの1.0 の数: {ones_count}\")\n",
    "    # print(f\"test_accuracies: {test_accuracies}\")\n",
    "    # t_ones_count = test_accuracies.count(1.0)\n",
    "    # print(f\"テストデータの1.0 の数: {t_ones_count}\")\n",
    "    # print(f\"test_accuracies: {sum(test_accuracies)}/{len(test_accuracies)}\")\n",
    "\n",
    "    # print(f\"推定値: {all_y_pred}\")\n",
    "\n",
    "    # plt.figure(figsize=(8, 6))\n",
    "    # sns.histplot(all_y_pred, kde=True, bins=20, color='blue')\n",
    "    # plt.xlabel(\"Predicted Probability\")\n",
    "    # plt.ylabel(\"Frequency\")\n",
    "    # plt.title(\"Distribution of Predicted Probabilities\")\n",
    "    # plt.grid()\n",
    "    # plt.show()\n",
    "    # 混同行列の描画を変更\n",
    "    cm = confusion_matrix(all_y_test, all_y_pred)\n",
    "    labels = [0,1]\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f'Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = range(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    # 各セルに値を表示\n",
    "    thresh = cm.max() / 2\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                    horizontalalignment=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.xlabel('Predict')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # ROC曲線の描画を変更\n",
    "    # fpr, tpr, _ = roc_curve(all_y_test, all_y_pred)\n",
    "    # roc_auc = auc(fpr, tpr)\n",
    "    # plt.figure()\n",
    "    # plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    # plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    # plt.xlabel('False Positive Rate')\n",
    "    # plt.ylabel('True Positive Rate')\n",
    "    # plt.title(f'ROC Curve for Task {n}')\n",
    "    # plt.legend(loc=\"lower right\")\n",
    "    # plt.show()\n",
    "    # F1スコアの計算を追加\n",
    "    # train_f1 = f1_score(y_train, y_train_pred_binary)\n",
    "    # True Positives, True Negatives, False Positives, False Negativesを取り出す\n",
    "    TP = cm[0, 0]\n",
    "    TN = cm[1, 1]\n",
    "    FP = cm[0, 1]\n",
    "    FN = cm[1, 0]\n",
    "    # 精度の計算\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    test_f1 = f1_score(all_y_test, all_y_pred)\n",
    "    print(f\"Test F1: {test_f1:.2f}\")\n",
    "    # 特徴量重要度の可視化\n",
    "    combined_importance = pd.concat(all_feature_importances)\n",
    "    aggregated_importance = combined_importance.groupby('Feature').mean().reset_index()\n",
    "    aggregated_importance = aggregated_importance.sort_values(by='Importance', ascending=False)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(aggregated_importance['Feature'], aggregated_importance['Importance'], color='skyblue')\n",
    "    plt.xlabel('Average Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Average Feature Importance Across All Models')\n",
    "    plt.gca().invert_yaxis()  # 特徴量を上から順に表示\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"Top 10 Features by Average Importance:\")\n",
    "    print(aggregated_importance.head(10))\n",
    "    \n",
    "    \n",
    "    \n",
    "#全員分の統合\n",
    "# 混同行列の描画\n",
    "print(\"総合評価\")\n",
    "all_cm = confusion_matrix(all_user_test, all_user_pred)\n",
    "labels = [0,1]\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(all_cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title(f'Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = range(len(labels))\n",
    "plt.xticks(tick_marks, labels, rotation=45)\n",
    "plt.yticks(tick_marks, labels)\n",
    "# 各セルに値を表示\n",
    "thresh = all_cm.max() / 2\n",
    "for i in range(all_cm.shape[0]):\n",
    "    for j in range(all_cm.shape[1]):\n",
    "        plt.text(j, i, format(all_cm[i, j], 'd'),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if all_cm[i, j] > thresh else \"black\")\n",
    "plt.xlabel('Predict')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "TP = all_cm[0, 0]\n",
    "TN = all_cm[1, 1]\n",
    "FP = all_cm[0, 1]\n",
    "FN = all_cm[1, 0]\n",
    "# 精度の計算\n",
    "all_accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "print(f\"Accuracy: {all_accuracy}\")\n",
    "all_f1 = f1_score(all_user_test, all_user_pred)\n",
    "print(f\"F1: {all_f1:.2f}\")\n",
    "\n",
    "# 特徴量重要度の可視化\n",
    "combined_all_importance = pd.concat(all_user_feature_importances)\n",
    "aggregated_all_importance = combined_all_importance.groupby('Feature').mean().reset_index()\n",
    "aggregated_all_importance = aggregated_all_importance.sort_values(by='Importance', ascending=False)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(aggregated_all_importance['Feature'], aggregated_all_importance['Importance'], color='skyblue')\n",
    "plt.xlabel('Average Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Average Feature Importance Across All Models')\n",
    "plt.gca().invert_yaxis()  # 特徴量を上から順に表示\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Top 10 Features by Average Importance:\")\n",
    "print(aggregated_all_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = [\n",
    "    0.879432624113, 0.909090909091, 0.885416666667, 0.921568627451, 0.944055944056,\n",
    "    0.844444444444, 0.885416666667, 0.911111111111, 0.956834532374, 0.950704225352,\n",
    "    0.913793103448, 0.842519685039, 0.929203539823, 0.816666666667, 0.944444444444,\n",
    "    0.867647058824, 0.93023255814, 0.941605839416, 0.918367346939, 0.969465648855,\n",
    "    0.935483870968, 0.931034482759, 0.888888888889, 0.896551724138, 0.915254237288,\n",
    "    0.918032786885, 0.85\n",
    "]\n",
    "# F1_score = [\n",
    "#     0.925110132159, 0.948979591837, 0.928104575163, 0.956989247312, 0.966666666667,\n",
    "#     0.893401015228, 0.917293233083, 0.953125, 0.97619047619, 0.97358490566,\n",
    "#     0.95, 0.913043478261, 0.961165048544, 0.855263157895, 0.968325791855,\n",
    "#     0.888888888889, 0.959459459459, 0.967479674797, 0.951219512195, 0.983333333333,\n",
    "#     0.964912280702, 0.95867768595, 0.925170068027, 0.933333333333, 0.952380952381,\n",
    "#     0.955357142857, 0.912621359223\n",
    "# ]\n",
    "precision = [\n",
    "    0.913043478261, 0.939393939394, 0.934210526316, 0.967391304348, 0.966666666667,\n",
    "    0.907216494845, 0.910447761194, 0.945736434109, 0.97619047619, 0.977272727273,\n",
    "    0.95, 0.905172413793, 0.961165048544, 0.844155844156, 0.963963963964,\n",
    "    0.923076923077, 0.959459459459, 0.959677419355, 0.951219512195, 0.983333333333,\n",
    "    0.964912280702, 0.95867768595, 0.931506849315, 0.945945945946, 0.943396226415,\n",
    "    0.946902654867, 0.959183673469\n",
    "]\n",
    "recall = [\n",
    "    0.9375, 0.958762886598, 0.922077922078, 0.946808510638, 0.966666666667,\n",
    "    0.88, 0.924242424242, 0.96062992126, 0.97619047619, 0.96992481203,\n",
    "    0.95, 0.921052631579, 0.961165048544, 0.866666666667, 0.972727272727,\n",
    "    0.857142857143, 0.959459459459, 0.975409836066, 0.951219512195, 0.983333333333,\n",
    "    0.964912280702, 0.95867768595, 0.918918918919, 0.921052631579, 0.961538461538,\n",
    "    0.963963963964, 0.87037037037\n",
    "]\n",
    "\n",
    "# Accuracy\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(accuracy, bins=10, color='skyblue', edgecolor='black')\n",
    "plt.title('Accuracy Distribution')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# # F1-Score\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# plt.hist(f1_score, bins=10, color='orange', edgecolor='black')\n",
    "# plt.title('F1-Score Distribution')\n",
    "# plt.xlabel('F1-Score')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n",
    "\n",
    "# Precision\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(precision, bins=10, color='green', edgecolor='black')\n",
    "plt.title('Precision Distribution')\n",
    "plt.xlabel('Precision')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Recall\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(recall, bins=10, color='red', edgecolor='black')\n",
    "plt.title('Recall Distribution')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル設計2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 回答品質の正常時と低下時の特徴量の分布をか確認\n",
    "# group_1 = refined_data[refined_data[”satisficing”] == 1]\n",
    "# group_0 = refined_data[refined_data[”satisficing”] == 0]\n",
    "\n",
    "# all_group_data = []\n",
    "\n",
    "# user_group = [\n",
    "# \"amemiya.naoki.ao9@naist.ac.jp\",\n",
    "# \"kasatani.hikaru.kd6@bs.naist.jp\",\n",
    "# \"syouta9592@i.softbank.jp\",\n",
    "# \"nishida.amane.mw6@ms.naist.jp\",\n",
    "# \"bessho.taisei.bu6@gmail.com\",\n",
    "# \"maya.sugi0102@gmail.com\",\n",
    "# \"sannzasannza@gmail.con\",\n",
    "# \"norilemon811@gmail.com\",\n",
    "# \"personal@muhammadalqaaf.com\",\n",
    "# \"niapotter@gmail.com\",\n",
    "# \"rubaet.ema@gmail.com\",\n",
    "# \"ito.mana.ik9@ms.naist.jp\",\n",
    "# \"oyebodeoyewale065@gmail.com\", \n",
    "# \"koyama.honami.kg7@naist.ac.jp\",\n",
    "# \"ilya.maisarah_binti_thariq.in9@naist.ac.jp\", \n",
    "# \"reru428@moimoi.re\",\n",
    "# \"sasaki.rina.su6@g.ext.naist.jp\", \n",
    "# \"marina.alki@tum.de\",\n",
    "# \"yamamoto.yukino.yv8@ms.naist.jp\", \n",
    "# \"li.siyuan.lu1@is.naist.jp\",\n",
    "# \"nakamura.kaito.nj3@is.naist.jp\", \n",
    "# \"zjnnyyj@outlook.jp\",\n",
    "# \"mspamungkas@icloud.com\", \n",
    "# \"m.kojima.kn6@naist.ac.jp\",\n",
    "# \"micnaist@gmail.com\",\n",
    "# \"nandakumar.ardra.na1@bs.naist.jp\",\n",
    "# \"tester11_1@gmail.com\"\n",
    "# ]\n",
    "\n",
    "# for sensor_id in unique_ids:\n",
    "#     file_path = f'/Users/shigeyuki-t/Desktop/Experiment_2/analysis/subject/{sensor_id}/feature_{sensor_id}.csv'\n",
    "#     df = pd.read_csv(file_path, index_col=0)\n",
    "#     # 必要な列のみ抽出\n",
    "#     remove_feature = ['dateTime', 'userEmail', 'screenW', 'screenH']\n",
    "#     df = df.drop(columns=remove_feature, errors='ignore')\n",
    "#     # df_mean = df.mean()  # 各被験者の特徴量の平均値を計算\n",
    "#     all_group_data.append(df)\n",
    "\n",
    "# # 特徴量データを1つのデータフレームに統合\n",
    "# feature_comparison_df = pd.DataFrame(all_group_data, index= unique_ids)\n",
    "# print(feature_comparison_df)\n",
    "# # good_user_group = (refined_data[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave One Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "#最初にkmeanで作ったクラスタ\n",
    "cluster1 = [\n",
    "    'nishida.amane.mw6@ms.naist.jp',\n",
    "    'norilemon811@gmail.com',\n",
    "    'personal@muhammadalqaaf.com',\n",
    "    'niapotter@gmail.com',\n",
    "    'rubaet.ema@gmail.com',\n",
    "    'oyebodeoyewale065@gmail.com',\n",
    "    'koyama.honami.kg7@naist.ac.jp',\n",
    "    'sasaki.rina.su6@g.ext.naist.jp',\n",
    "    'marina.alki@tum.de',\n",
    "    'nandakumar.ardra.na1@bs.naist.jp'\n",
    "]\n",
    "\n",
    "cluster2 = [\n",
    "    'amemiya.naoki.ao9@naist.ac.jp',\n",
    "    'kasatani.hikaru.kd6@bs.naist.jp',\n",
    "    'syouta9592@i.softbank.jp',\n",
    "    'bessho.taisei.bu6@gmail.com',\n",
    "    'maya.sugi0102@gmail.com',\n",
    "    'norilemon811@gmail.com',\n",
    "    'personal@muhammadalqaaf.com',\n",
    "    'niapotter@gmail.com',\n",
    "    'koyama.honami.kg7@naist.ac.jp',\n",
    "    'ilya.maisarah_binti_thariq.in9@naist.ac.jp',\n",
    "    'reru428@moimoi.re',\n",
    "    'sasaki.rina.su6@g.ext.naist.jp',\n",
    "    'marina.alki@tum.de',\n",
    "    'yamamoto.yukino.yv8@ms.naist.jp',\n",
    "    'li.siyuan.lu1@is.naist.jp',\n",
    "    'zjnnyyj@outlook.jp',\n",
    "    'tester11_1@gmail.com'\n",
    "    ]\n",
    "\n",
    "cluster3 = [\n",
    "\"amemiya.naoki.ao9@naist.ac.jp\",\n",
    "\"kasatani.hikaru.kd6@bs.naist.jp\",\n",
    "\"syouta9592@i.softbank.jp\",\n",
    "\"nishida.amane.mw6@ms.naist.jp\",\n",
    "\"bessho.taisei.bu6@gmail.com\",\n",
    "\"maya.sugi0102@gmail.com\",\n",
    "\"sannzasannza@gmail.con\",\n",
    "\"norilemon811@gmail.com\",\n",
    "\"personal@muhammadalqaaf.com\",\n",
    "\"niapotter@gmail.com\",\n",
    "\"rubaet.ema@gmail.com\",\n",
    "\"ito.mana.ik9@ms.naist.jp\",\n",
    "\"oyebodeoyewale065@gmail.com\",\n",
    "\"koyama.honami.kg7@naist.ac.jp\",\n",
    "\"ilya.maisarah_binti_thariq.in9@naist.ac.jp\",\n",
    "\"reru428@moimoi.re\",\n",
    "\"sasaki.rina.su6@g.ext.naist.jp\",\n",
    "\"marina.alki@tum.de\",\n",
    "\"yamamoto.yukino.yv8@ms.naist.jp\",\n",
    "\"li.siyuan.lu1@is.naist.jp\",\n",
    "\"nakamura.kaito.nj3@is.naist.jp\",\n",
    "\"zjnnyyj@outlook.jp\",\n",
    "\"mspamungkas@icloud.com\",\n",
    "\"m.kojima.kn6@naist.ac.jp\",\n",
    "\"micnaist@gmail.com\",\n",
    "\"nandakumar.ardra.na1@bs.naist.jp\",\n",
    "\"tester11_1@gmail.com\"\n",
    "]\n",
    "\n",
    "#DBSCANで作ったクラスタ\n",
    "cluster0 = [\n",
    "    'amemiya.naoki.ao9@naist.ac.jp', 'kasatani.hikaru.kd6@bs.naist.jp'#, 'bessho.taisei.bu6@gmail.com', 'sannzasannza@gmail.con', 'norilemon811@gmail.com', 'personal@muhammadalqaaf.com', 'niapotter@gmail.com', 'oyebodeoyewale065@gmail.com', 'koyama.honami.kg7@naist.ac.jp', 'ilya.maisarah_binti_thariq.in9@naist.ac.jp', 'reru428@moimoi.re', 'sasaki.rina.su6@g.ext.naist.jp', 'marina.alki@tum.de', 'yamamoto.yukino.yv8@ms.naist.jp', 'nakamura.kaito.nj3@is.naist.jp', 'nandakumar.ardra.na1@bs.naist.jp'\n",
    "]\n",
    "final_predictions = {}\n",
    "\n",
    "all_lopo_user_test = []\n",
    "all_lopo_user_pred = []\n",
    "\n",
    "for test_id in cluster3:\n",
    "    cross_predictions = []\n",
    "    testfile_path =  f'/Users/shigeyuki-t/Desktop/Experiment_2/analysis/subject/{test_id}/feature_{test_id}.csv'\n",
    "    test_df = pd.read_csv(testfile_path)\n",
    "    test_df = test_df.drop(columns = ['userEmail', 'dateTime', 'Unnamed: 0'])\n",
    "    # print(test_df.columns)\n",
    "\n",
    "    roc_df = test_df.copy()\n",
    "\n",
    "    lopo_user_test = []\n",
    "    lopo_user_pred = []\n",
    "\n",
    "    # 各modelを使って予測し、isCorrectを推定\n",
    "    for sensor_id in [user for user in cluster3 if user != test_id]:\n",
    "    # for sensor_id in cluster1:\n",
    "        # print(test_id,sensor_id)\n",
    "        model_path = '/Users/shigeyuki-t/Desktop/Experiment_2/analysis/subject/' + f'{sensor_id}'\n",
    "        # print(model_path)\n",
    "        for n in range(1,151):\n",
    "            try:\n",
    "                model_file = os.path.join(model_path, f'model_{n}.txt')\n",
    "                if not os.path.exists(model_file):\n",
    "                    continue\n",
    "\n",
    "                load_model = lgb.Booster(model_file=model_file)\n",
    "\n",
    "                # モデルが期待する特徴量の確認\n",
    "                expected_columns = load_model.feature_name()\n",
    "                # print(f\"Expected columns for model {n}: {expected_columns}\")\n",
    "                # print(f\"Test data columns: {test_df.columns}\")\n",
    "                # モデルに必要な特徴量がテストデータに含まれているか確認\n",
    "                missing_columns = list(set(expected_columns) - set(test_df.columns))\n",
    "                extra_columns = list(set(test_df.columns) - set(expected_columns))\n",
    "                # print(f\"Missing columns in test data: {missing_columns}\")\n",
    "                # print(f\"Extra columns in test data: {extra_columns}\")\n",
    "                # 足りないカラムを追加する\n",
    "                for column in missing_columns:\n",
    "                    test_df[column] = 0\n",
    "                # 余分なカラムを削除する\n",
    "                test_df = test_df.drop(columns=extra_columns)\n",
    "                # 特徴量の順番をモデルに合わせて並べ替える\n",
    "                test_df = test_df[expected_columns]\n",
    "                zikkenn = test_df[expected_columns]\n",
    "\n",
    "                pred = load_model.predict(zikkenn, num_iteration=load_model.best_iteration)\n",
    "                # print(pred)\n",
    "                cross_predictions.append(pred)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load model {n}: {e}\")\n",
    "                continue\n",
    "\n",
    "    if cross_predictions:\n",
    "        # cross_predictionsをnumpy配列に変換\n",
    "        predictions_array = np.array(cross_predictions)  # shape: (モデル数, データ数)\n",
    "\n",
    "        # 各データポイント（列）の平均を計算\n",
    "        mean_pred = np.mean(predictions_array, axis=0)\n",
    "        # print(test_df.columns)  # テストデータのカラム名を表示\n",
    "\n",
    "        # ROC曲線を使用して最適な閾値を計算\n",
    "        if 'satisficing' in roc_df.columns:\n",
    "            actual = roc_df['satisficing'].values  # 実際のターゲット値\n",
    "\n",
    "            # # Precision-Recall 曲線の計算\n",
    "            # precision, recall, thresholds = precision_recall_curve(actual, mean_pred)\n",
    "            # # TP を最大化しつつ、FN を最小化するため、高い Recall を確保\n",
    "            # desired_recall = 0.85  # Recall が 85%以上になるしきい値を選ぶ\n",
    "            # valid_idx = np.where(recall[:-1] >= desired_recall)[0]  # thresholds に対応するインデックスを取得\n",
    "            # if len(valid_idx) > 0:\n",
    "            #     optimal_idx = valid_idx[-1]  # Recall が 85%以上を満たす中で最も高い閾値を選ぶ\n",
    "            #     optimal_threshold = thresholds[optimal_idx]\n",
    "            # else:\n",
    "            #     optimal_threshold = 0.5  # 適切な閾値がない場合はデフォルト\n",
    "            # print(f\"Optimal threshold from Precision-Recall curve: {optimal_threshold:.4f}\")\n",
    "\n",
    "            fpr, tpr, thresholds = roc_curve(actual, mean_pred)\n",
    "            optimal_idx = np.argmax(tpr - fpr)  # TPR - FPR が最大となる閾値\n",
    "            optimal_threshold = thresholds[optimal_idx]\n",
    "            print(f\"Optimal threshold from ROC curve: {optimal_threshold:.4f}\")\n",
    "\n",
    "            # 動的閾値を使用した予測結果\n",
    "            # re_mean_pred = [1 if p > optimal_threshold else 0 for p in mean_pred]\n",
    "            re_mean_pred = [1 if p > 0.17 else 0 for p in mean_pred]\n",
    "            # predict_isCorrect カラムを追加\n",
    "            roc_df[\"predict_satisficing\"] = re_mean_pred\n",
    "\n",
    "            # 精度を計算\n",
    "            # correct_predictions = (roc_df[\"predict_satisficing\"] == roc_df[\"satisficing\"]).sum()\n",
    "            # accuracy = correct_predictions / len(roc_df)\n",
    "            # print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "            # # AUCの計算\n",
    "            # roc_auc = auc(fpr, tpr)\n",
    "            # print(f\"AUC: {roc_auc:.4f}\")\n",
    "\n",
    "            # F1スコアの計算\n",
    "            f1 = f1_score(roc_df[\"satisficing\"], roc_df[\"predict_satisficing\"])\n",
    "            print(f\"F1 Score: {f1:.4f}\")\n",
    "            # print(lopo_user_test)\n",
    "            # print(lopo_user_pred)\n",
    "\n",
    "            # \"task\"ごとのユニークなリストを取得\n",
    "            unique_tasks = roc_df[\"task\"].unique()\n",
    "            # print(\"unique_tasks:\", unique_tasks)\n",
    "            # print(\"zikkenn のカラム一覧:\", zikkenn.columns)\n",
    "            # タスクごとの代表値を取得\n",
    "            for task in unique_tasks:\n",
    "                # 該当タスクのデータを抽出\n",
    "                task_data = roc_df[roc_df[\"task\"] == task]\n",
    "                # actual（isCorrect）の代表値 → 平均または最頻値\n",
    "                actual_value = task_data[\"satisficing\"].mean()  # 平均値を使用\n",
    "                # actual_value = task_data[\"isCorrect\"].mode()[0]  # 最頻値を使う場合\n",
    "                # re_mean_pred（予測値）の代表値 → 平均値を使用\n",
    "                pred_value = task_data[\"predict_satisficing\"].mean()\n",
    "                # リストに追加\n",
    "                # print(f\"追加前の lopo_user_test: {lopo_user_test}\")\n",
    "                lopo_user_test.append(actual_value)\n",
    "                all_lopo_user_test.append(actual_value)\n",
    "                # print(f\"追加後の lopo_user_test: {lopo_user_test}\")\n",
    "                lopo_user_pred.append(pred_value)\n",
    "                all_lopo_user_pred.append(pred_value)\n",
    "                # print(\"実行されてる\")\n",
    "\n",
    "            # 混同行列の計算\n",
    "            lopo_user_pred = np.array(lopo_user_pred , dtype=int)\n",
    "            lopo_user_test = np.array(lopo_user_test, dtype=int)\n",
    "            cm = confusion_matrix(lopo_user_test, lopo_user_pred)\n",
    "            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['0', '1'])\n",
    "            # print(f\"Confusion Matrix:\\n{cm}\")\n",
    "            # 最終結果を保存\n",
    "            final_predictions[test_id] = {\n",
    "                \"accuracy\": accuracy,\n",
    "                \"optimal_threshold\": optimal_threshold,\n",
    "                \"confusion_matrix\": cm,\n",
    "            }\n",
    "\n",
    "            # 混同行列の可視化\n",
    "            disp.plot(cmap='Blues', values_format='d')\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.show()\n",
    "\n",
    "            # # ROC曲線の描画\n",
    "            # plt.figure(figsize=(8, 6))\n",
    "            # plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "            # plt.scatter(fpr[optimal_idx], tpr[optimal_idx], color='red', label=f\"Optimal Threshold = {optimal_threshold:.2f}\")\n",
    "            # plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "            # plt.xlabel('False Positive Rate')\n",
    "            # plt.ylabel('True Positive Rate')\n",
    "            # plt.title('ROC Curve')\n",
    "            # plt.legend()\n",
    "            # plt.grid()\n",
    "            # plt.show()\n",
    "\n",
    "            # 全体の予測分布を可視化\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.hist(mean_pred, bins=30, alpha=0.7, label='Predictions')\n",
    "            plt.axvline(optimal_threshold, color='red', linestyle='--', label='Optimal Threshold')\n",
    "            plt.xlabel('Prediction Probability')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title('Prediction Probability Distribution')\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "\n",
    "            print(\"Dynamic threshold predictions:\", re_mean_pred)\n",
    "        else:\n",
    "            print(\"No 'isCorrect' column found in test data; skipping ROC threshold calculation.\")\n",
    "    else:\n",
    "        print(\"No predictions available.\")\n",
    "\n",
    "    if \"satisficing\" in roc_df.columns and \"predict_satisficing\" in roc_df.columns:\n",
    "        # isCorrect の値ごとのカウント\n",
    "        is_correct_counts = roc_df[\"satisficing\"].value_counts()\n",
    "        print(\"Counts in isCorrect column:\")\n",
    "        print(is_correct_counts)\n",
    "\n",
    "        # predict_isCorrect の値ごとのカウント\n",
    "        predict_is_correct_counts = roc_df[\"predict_satisficing\"].value_counts()\n",
    "        print(\"\\nCounts in predict_isCorrect column:\")\n",
    "        print(predict_is_correct_counts)\n",
    "\n",
    "        # 比較を分かりやすく出力\n",
    "        print(\"\\nComparison of counts:\")\n",
    "        comparison = pd.DataFrame({\n",
    "            \"satisficing\": is_correct_counts,\n",
    "            \"predict_satisficing\": predict_is_correct_counts\n",
    "        }).fillna(0)  # 欠損値を 0 で補完\n",
    "        print(comparison)\n",
    "    else:\n",
    "        print(\"Either isCorrect or predict_isCorrect column is missing.\")\n",
    "\n",
    "for test_id, result in final_predictions.items():\n",
    "    print(f\"Results for {test_id}:\")\n",
    "    print(f\"Accuracy: {result['accuracy']:.4f}\")\n",
    "    print(f\"Optimal Threshold: {result['optimal_threshold']:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(result[\"confusion_matrix\"])\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "if all_lopo_user_test and all_lopo_user_pred:\n",
    "    print(\"総合評価\")\n",
    "    # # 予測値（lopo_user_pred）を0/1のラベルに変換\n",
    "    all_lopo_user_pred_binary = [1 if p > 0.5 else 0 for p in all_lopo_user_pred]\n",
    "\n",
    "    # # データ型を整数に変換（明示的にintに統一）\n",
    "    all_lopo_user_test = np.nan_to_num(all_lopo_user_test, nan=0)\n",
    "    all_lopo_user_test = np.array(all_lopo_user_test, dtype=int)\n",
    "    all_lopo_user_pred_binary = np.array(all_lopo_user_pred_binary, dtype=int)\n",
    "\n",
    "    # 予測と実際のラベルが0と1のみに含まれているか確認\n",
    "    print(set(all_lopo_user_test))\n",
    "    print(set(all_lopo_user_pred_binary))\n",
    "\n",
    "    # 不要なラベルをフィルタリング\n",
    "    all_lopo_user_test = [x for x in all_lopo_user_test if x in [0, 1]]\n",
    "    all_lopo_user_pred_binary = [x for x in all_lopo_user_pred_binary if x in [0, 1]]\n",
    "\n",
    "    # 混同行列を計算\n",
    "    cm_total = confusion_matrix(all_lopo_user_test, all_lopo_user_pred_binary)\n",
    "    disp_total = ConfusionMatrixDisplay(confusion_matrix=cm_total, display_labels=['0', '1'])\n",
    "    disp_total.plot(cmap='Blues', values_format='d')\n",
    "    plt.xlabel('Predict')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = [\n",
    "0.689189189189,\n",
    "0.736486486486,\n",
    "0.679611650485,\n",
    "0.837837837838,\n",
    "0.641891891892,\n",
    "0.550724637681,\n",
    "0.602739726027,\n",
    "0.810810810811,\n",
    "0.763513513514,\n",
    "0.790540540541,\n",
    "0.655405405405,\n",
    "0.797297297297,\n",
    "0.858108108108,\n",
    "0.540540540541,\n",
    "0.777027027027,\n",
    "0.630136986301,\n",
    "0.783333333333,\n",
    "0.871621621622,\n",
    "0.655172413793,\n",
    "0.695945945946,\n",
    "0.878787878788,\n",
    "0.918918918919,\n",
    "0.641891891892,\n",
    "0.716216216216,\n",
    "0.682432432432,\n",
    "0.462585034014,\n",
    "0.837837837838,\n",
    "]\n",
    "F1_score = [\n",
    "0.801724137931,\n",
    "0.843373493976,\n",
    "0.8,\n",
    "0.911764705882,\n",
    "0.75117370892,\n",
    "0.686868686869,\n",
    "0.743362831858,\n",
    "0.893129770992,\n",
    "0.853556485356,\n",
    "0.880308880309,\n",
    "0.786610878661,\n",
    "0.878048780488,\n",
    "0.923636363636,\n",
    "0.701754385965,\n",
    "0.859574468085,\n",
    "0.773109243697,\n",
    "0.878504672897,\n",
    "0.923694779116,\n",
    "0.782608695652,\n",
    "0.819277108434,\n",
    "0.935483870968,\n",
    "0.952755905512,\n",
    "0.781893004115,\n",
    "0.829268292683,\n",
    "0.801687763713,\n",
    "0.632558139535,\n",
    "0.911764705882,\n",
    "]\n",
    "precision = [\n",
    "0.781512605042,\n",
    "0.905172413793,\n",
    "0.80487804878,\n",
    "0.932330827068,\n",
    "0.879120879121,\n",
    "0.708333333333,\n",
    "0.75,\n",
    "0.951219512195,\n",
    "0.918918918919,\n",
    "0.934426229508,\n",
    "0.87037037037,\n",
    "0.830769230769,\n",
    "0.920289855072,\n",
    "0.634920634921,\n",
    "0.961904761905,\n",
    "0.630136986301,\n",
    "0.878504672897,\n",
    "0.912698412698,\n",
    "0.837209302326,\n",
    "0.902654867257,\n",
    "0.935483870968,\n",
    "0.916666666667,\n",
    "0.77868852459,\n",
    "0.784615384615,\n",
    "0.904761904762,\n",
    "0.85,\n",
    "0.946564885496,\n",
    "]\n",
    "recall = [\n",
    "0.823008849558,\n",
    "0.789473684211,\n",
    "0.795180722892,\n",
    "0.892086330935,\n",
    "0.655737704918,\n",
    "0.666666666667,\n",
    "0.736842105263,\n",
    "0.841726618705,\n",
    "0.796875,\n",
    "0.832116788321,\n",
    "0.717557251908,\n",
    "0.931034482759,\n",
    "0.92700729927,\n",
    "0.78431372549,\n",
    "0.776923076923,\n",
    "1,\n",
    "0.878504672897,\n",
    "0.934959349593,\n",
    "0.734693877551,\n",
    "0.75,\n",
    "0.935483870968,\n",
    "0.991803278689,\n",
    "0.785123966942,\n",
    "0.879310344828,\n",
    "0.719696969697,\n",
    "0.503703703704,\n",
    "0.879432624113,\n",
    "]\n",
    "\n",
    "# Accuracy\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(accuracy, bins=10, color='skyblue', edgecolor='black')\n",
    "plt.title('Accuracy Distribution')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# F1-Score\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(F1_score, bins=10, color='orange', edgecolor='black')\n",
    "plt.title('F1-Score Distribution')\n",
    "plt.xlabel('F1-Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Precision\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(precision, bins=10, color='green', edgecolor='black')\n",
    "plt.title('Precision Distribution')\n",
    "plt.xlabel('Precision')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Recall\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(recall, bins=10, color='red', edgecolor='black')\n",
    "plt.title('Recall Distribution')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import f1_score, roc_curve, auc\n",
    "\n",
    "# cross_predictions = []\n",
    "# clustered_groups = [\n",
    "#     'nishida.amane.mw6@ms.naist.jp', \n",
    "#     'norilemon811@gmail.com', \n",
    "#     'personal@muhammadalqaaf.com', \n",
    "#     'niapotter@gmail.com', \n",
    "#     'rubaet.ema@gmail.com', \n",
    "#     'oyebodeoyewale065@gmail.com', \n",
    "#     'koyama.honami.kg7@naist.ac.jp', \n",
    "#     'sasaki.rina.su6@g.ext.naist.jp', \n",
    "#     'marina.alki@tum.de', \n",
    "#     # 'nandakumar.ardra.na1@bs.naist.jp'\n",
    "# ]\n",
    "\n",
    "# test_id =  'nandakumar.ardra.na1@bs.naist.jp'\n",
    "# testfile_path = f'/Users/shigeyuki-t/Desktop/Experiment_2/analysis/subject/{test_id}/feature_{test_id}.csv'\n",
    "\n",
    "# roc_df = pd.read_csv(testfile_path)\n",
    "# roc_df = roc_df.drop(columns=['userEmail', 'dateTime', 'Unnamed: 0'])\n",
    "\n",
    "# print(roc_df.columns)\n",
    "\n",
    "# # タスクごとに予測を実行\n",
    "# tasks = roc_df[\"task\"].unique()  # ユニークな task 値を取得\n",
    "\n",
    "# for task in tasks:\n",
    "#     # 現在の task に対応するデータを取得\n",
    "#     task_data = roc_df[roc_df[\"task\"] == task].copy()\n",
    "#     # task_data = roc_df[roc_df[\"task\"].isin(range(task-10, task+1))].copy()ここじゃない\n",
    "\n",
    "#     task_predictions = []  # このタスクの全モデルの予測を保存\n",
    "\n",
    "#     for sensor_id in clustered_groups:\n",
    "#         model_path = f'/Users/shigeyuki-t/Desktop/Experiment_2/analysis/subject/{sensor_id}'\n",
    "# #         print(model_path)\n",
    "#         for n in range(1, 151):\n",
    "#             try:\n",
    "#                 model_file = os.path.join(model_path, f'model_{n}.txt')\n",
    "#                 if not os.path.exists(model_file):\n",
    "#                     continue\n",
    "\n",
    "#                 load_model = lgb.Booster(model_file=model_file)\n",
    "\n",
    "#                 # モデルが期待する特徴量の確認\n",
    "#                 expected_columns = load_model.feature_name()\n",
    "\n",
    "#                 # モデルに必要な特徴量を補完\n",
    "#                 missing_columns = list(set(expected_columns) - set(task_data.columns))\n",
    "#                 for column in missing_columns:\n",
    "#                     task_data[column] = 0\n",
    "#                 task_data = task_data[expected_columns]  # 特徴量の順番をモデルに合わせる\n",
    "\n",
    "#                 # 予測を実行\n",
    "#                 pred = load_model.predict(task_data, num_iteration=load_model.best_iteration)\n",
    "#                 task_predictions.append(pred)\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Failed to load model {n}: {e}\")\n",
    "#                 continue\n",
    "\n",
    "#     # タスク単位の平均予測値を計算\n",
    "#     if task_predictions:\n",
    "#         task_predictions_array = np.array(task_predictions)  # shape: (モデル数, データ数)\n",
    "#         mean_pred_task = np.mean(task_predictions_array, axis=0)\n",
    "\n",
    "#         # タスクデータに予測値を反映\n",
    "#         roc_df.loc[roc_df[\"task\"] == task, \"predict_isCorrect\"] = mean_pred_task\n",
    "#     else:\n",
    "#         print(f\"No predictions available for task {task}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ROC曲線を使用して最適な閾値を計算\n",
    "# if \"isCorrect\" in roc_df.columns and \"predict_isCorrect\" in roc_df.columns:\n",
    "#     actual = roc_df[\"isCorrect\"].values\n",
    "#     predictions = roc_df[\"predict_isCorrect\"].values\n",
    "#     fpr, tpr, thresholds = roc_curve(actual, predictions)\n",
    "#     optimal_idx = np.argmax(tpr - fpr)  # TPR - FPR が最大となる閾値\n",
    "# #     # FPRが0.2以下の範囲でTPRが最大となる閾値を選択\n",
    "# #     acceptable_fpr = 0.6\n",
    "# #     filtered_indices = np.where(fpr <= acceptable_fpr)[0]\n",
    "# #     optimal_idx = filtered_indices[np.argmax(tpr[filtered_indices])]\n",
    "#     optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "#     print(f\"Optimal threshold from ROC curve: {optimal_threshold:.4f}\")\n",
    "\n",
    "#     # 閾値を使用して最終予測を計算\n",
    "#     roc_df[\"final_predict_isCorrect\"] = roc_df[\"predict_isCorrect\"].apply(lambda x: 1 if x > optimal_threshold else 0)\n",
    "\n",
    "#     # 精度を計算\n",
    "#     correct_predictions = (roc_df[\"final_predict_isCorrect\"] == roc_df[\"isCorrect\"]).sum()\n",
    "#     accuracy = correct_predictions / len(roc_df)\n",
    "#     print(f\"predict :{roc_df['final_predict_isCorrect']}\")\n",
    "#     print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "#     # AUCの計算\n",
    "#     roc_auc = auc(fpr, tpr)\n",
    "#     print(f\"AUC: {roc_auc:.4f}\")\n",
    "          \n",
    "#     # F1スコアの計算\n",
    "#     f1 = f1_score(actual, roc_df[\"final_predict_isCorrect\"])\n",
    "#     print(f\"F1 Score: {f1:.4f}\")\n",
    "          \n",
    "#     # 混同行列の計算\n",
    "#     conf_matrix = confusion_matrix(actual, roc_df[\"final_predict_isCorrect\"])\n",
    "#     print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "          \n",
    "#     # 混同行列の可視化\n",
    "#     plt.figure(figsize=(6, 5))\n",
    "#     sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Incorrect\", \"Correct\"], yticklabels=[\"Incorrect\", \"Correct\"])\n",
    "#     plt.xlabel('Predicted Label')\n",
    "#     plt.ylabel('True Label')\n",
    "#     plt.title('Confusion Matrix')\n",
    "#     plt.show()\n",
    "\n",
    "#     # ROC曲線の描画\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "#     plt.scatter(fpr[optimal_idx], tpr[optimal_idx], color='red', label=f\"Optimal Threshold = {optimal_threshold:.2f}\")\n",
    "#     plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.title('ROC Curve')\n",
    "#     plt.legend()\n",
    "#     plt.grid()\n",
    "#     plt.show()\n",
    "\n",
    "#     # 全体の予測分布を可視化\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     plt.hist(predictions, bins=30, alpha=0.7, label='Predictions')\n",
    "#     plt.axvline(optimal_threshold, color='red', linestyle='--', label='Optimal Threshold')\n",
    "#     plt.xlabel('Prediction Probability')\n",
    "#     plt.ylabel('Frequency')\n",
    "#     plt.title('Prediction Probability Distribution')\n",
    "#     plt.legend()\n",
    "#     plt.grid()\n",
    "#     plt.show()\n",
    "\n",
    "#     print(\"Predictions with dynamic threshold are saved in `final_predict_isCorrect`.\")\n",
    "# else:\n",
    "#     print(\"Either `isCorrect` or `predict_isCorrect` column is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多数決"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 各モデルの予測結果を格納するリスト\n",
    "# cross_predictions = []\n",
    "\n",
    "# for sensor_id in group:\n",
    "#     model_path = '/Users/shigeyuki-t/Desktop/Experiment_2/analysis/subject/' + f'{sensor_id}'\n",
    "#     print(model_path)\n",
    "#     for n in range(1,151):\n",
    "#         try:         \n",
    "#             model_file = os.path.join(model_path, f'model_{n}.txt')\n",
    "#             if not os.path.exists(model_file):\n",
    "#                 continue\n",
    "            \n",
    "#             load_model = lgb.Booster(model_file=model_file)\n",
    "#             zikkenn = test_df[features].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "#             pred = load_model.predict(zikkenn, num_iteration=model.best_iteration)\n",
    "#             cross_predictions.append(pred)\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Failed to load model {n}: {e}\")\n",
    "#             continue\n",
    "\n",
    "# # 多数決で予測\n",
    "# if cross_predictions:\n",
    "#     predictions_array = np.array(cross_predictions)  # shape: (モデル数, データ数)\n",
    "#     # 各データポイントごとに予測結果を多数決で集約\n",
    "#     majority_vote = np.round(np.mean(predictions_array, axis=0))\n",
    "#     majority_vote = [1 if p > 0.5 else 0 for p in majority_vote]\n",
    "#     print(\"多数決結果:\", majority_vote)\n",
    "# else:\n",
    "#     print(\"No predictions available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model results\n",
    "accuracies = [\n",
    "    0.6043, 0.5302, 0.4614, 0.4573, 0.4530, \n",
    "    0.5676, 0.3960, 0.5630, 0.5636, 0.5224\n",
    "]\n",
    "f1_scores = [\n",
    "    0.6837, 0.6156, 0.3173, 0.5157, 0.5072, \n",
    "    0.6954, 0.5010, 0.6449, 0.6367, 0.6006\n",
    "]\n",
    "\n",
    "# Calculate means\n",
    "mean_accuracy = sum(accuracies) / len(accuracies)\n",
    "mean_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "mean_accuracy, mean_f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
